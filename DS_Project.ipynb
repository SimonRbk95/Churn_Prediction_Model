{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a13b87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier # Import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Import feature and tree visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "from matplotlib import style\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a39bb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "start_features = ['Age', 'BusinessTravel', 'DailyRate', 'Department',\n",
    "       'DistanceFromHome', 'Education', 'EducationField',\n",
    "        'EnvironmentSatisfaction', 'Gender', 'HourlyRate',\n",
    "       'JobInvolvement', 'JobLevel', 'JobRole', 'JobSatisfaction',\n",
    "       'MaritalStatus', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked',\n",
    "        'OverTime', 'PercentSalaryHike', 'PerformanceRating',\n",
    "       'RelationshipSatisfaction', 'StockOptionLevel',\n",
    "       'TotalWorkingYears', 'TrainingTimesLastYear', 'WorkLifeBalance',\n",
    "       'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion',\n",
    "       'YearsWithCurrManager']\n",
    "target = \"Attrition\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "addcf1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # create dataframe with project's data set\n",
    "    df = pd.read_csv('HR.csv')\n",
    "    \n",
    "    # clean the data\n",
    "    clean_df = clean_data(df)\n",
    "\n",
    "    # create the model\n",
    "    # takes \"decisiontree\" or \"randomforest\" as its second argument\n",
    "    create_model(df, clean_df, \"decisiontree\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "006372e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_categoricals(df):\n",
    "    cat_features = []\n",
    "    \n",
    "    # extract a list of all categorical features for feature engineering except target column\n",
    "    for col_header in df.columns:\n",
    "        if col_header != target and df.dtypes[col_header] == object:\n",
    "            cat_features.append(col_header)         \n",
    "    return cat_features\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43892ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    # drop columns that are not needed\n",
    "    for col_header in df.columns:\n",
    "        if col_header not in start_features and col_header != target:\n",
    "            df.drop(col_header, axis=1, inplace=True)\n",
    "            \n",
    "    # drop rows that contain at least one empty cell\n",
    "    df.dropna(axis='rows', thresh=1)\n",
    "    \n",
    "    # feature enigneering for categorical non-binary data\n",
    "    clean_df = pd.get_dummies(data=df, columns=extract_categoricals(df))\n",
    "    \n",
    "    return clean_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7b16df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(clean_df):\n",
    "    \n",
    "    # get target variable data and exclude it from the features\n",
    "    y = clean_df.pop(target)\n",
    "      \n",
    "    # get new feature list\n",
    "    features = clean_df.columns.tolist()\n",
    "    \n",
    "    # get feature data\n",
    "    X = clean_df[features] # Features Matrix\n",
    "    \n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8db0ca8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_oversampling(X_train, y_train):\n",
    "    # https://www.kaggle.com/code/residentmario/undersampling-and-oversampling-imbalanced-data/notebook\n",
    "    #adjust target data imbalance by over/undersampling\n",
    "    rus = RandomOverSampler(random_state=0)\n",
    "    rus.fit(X_train, y_train)\n",
    "    X_resampled, y_resampled = rus.fit_resample(X_train, y_train)\n",
    "    print(y_resampled.value_counts())\n",
    "    return X_resampled, y_resampled\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5683187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_undersampling(X_train, y_train):\n",
    "    # https://www.kaggle.com/code/residentmario/undersampling-and-oversampling-imbalanced-data/notebook\n",
    "    #adjust target data imbalance by over/undersampling\n",
    "    rus = RandomUnderSampler(random_state=0)\n",
    "    rus.fit(X_train, y_train)\n",
    "    X_resampled, y_resampled = rus.fit_resample(X_train, y_train)\n",
    "    print(y_resampled.value_counts())\n",
    "    return X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f9c8f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(df, clean_df, string):\n",
    "    # get features matrix X and target variable y\n",
    "    X, y = feature_selection(clean_df)\n",
    "    \n",
    "    # Split dataset into training set and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "  \n",
    "    # possible imbalance adjustment\n",
    "    # X_train, y_train = data_oversampling(X_train, y_train)\n",
    "    \n",
    "    if string == \"decisiontree\":\n",
    "        # Create Decision Tree classifer object\n",
    "        # use criterion=entropy as attribute or default gini\n",
    "        clf = DecisionTreeClassifier(criterion='entropy', max_depth=3)\n",
    "        print(\"Decision Tree: \")\n",
    "    elif string == \"randomforest\":\n",
    "        # Create random forest object\n",
    "        # use criterion=entropy as attribute or default gini\n",
    "        clf = RandomForestClassifier(criterion='gini', max_depth=30, n_estimators=200)\n",
    "        print(\"Random Forest: \")\n",
    "    else:\n",
    "        print(\"wrong input for create_model function.\")\n",
    "        return\n",
    "\n",
    "    # Train the model Classifer\n",
    "    clf = clf.fit(X_train,y_train)\n",
    "    \n",
    "    # Predict the response for test dataset\n",
    "    y_pred = clf.predict_proba(X_test)\n",
    "    print(y_pred)\n",
    "    \n",
    "    # print model accuracy to terminal\n",
    "    # model_accuracy(y_test, y_pred)\n",
    "    \n",
    "    # plot tree if applicable\n",
    "    if string == \"decisiontree\":\n",
    "        plot_tree(clf, X.columns.tolist())\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32599322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tree(clf, features):\n",
    "    \n",
    "    # Setting dpi = 300 to make image clearer than default\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(30,30), dpi=300)\n",
    "\n",
    "    tree.plot_tree(clf,\n",
    "           feature_names=features,\n",
    "           class_names=[\"No\",\"Yes\"],\n",
    "           filled=True,\n",
    "           fontsize=15);\n",
    "\n",
    "    fig.savefig('tree.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d672a110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_accuracy(y_test, y_pred):\n",
    "    print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9664777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_imbalance():\n",
    "    # visualize imbalance of Attrition data\n",
    "    print(y.value_counts())\n",
    "    pass\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0818852c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance():\n",
    "    # visualize feature importance based on spefici metric\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfc8e62",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# test\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "179def6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/simonrahenbrock/Documents/DS_project'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f939da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
